{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "demo.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AliceInHunterland/colab/blob/main/SpeechSplit_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfb_IKpnk9cC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "087de211-0433-4459-948e-33169ea25fc8"
      },
      "source": [
        "!git clone https://github.com/auspicious3000/SpeechSplit.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'SpeechSplit'...\n",
            "remote: Enumerating objects: 37, done.\u001b[K\n",
            "remote: Counting objects: 100% (37/37), done.\u001b[K\n",
            "remote: Compressing objects: 100% (27/27), done.\u001b[K\n",
            "remote: Total 37 (delta 12), reused 34 (delta 9), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (37/37), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6kqtyOGlOpM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bb310f9-38ff-45df-c4f2-d567c2c27fa0"
      },
      "source": [
        "cd SpeechSplit/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/SpeechSplit\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yq-aDHmql0KX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a776cf8b-8774-4be6-e318-88b4e9d3537c"
      },
      "source": [
        "cd assets/"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/SpeechSplit/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrJkJYnunDV1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8e7aef0-01d4-4565-8f19-6e64266acfbf"
      },
      "source": [
        "!gdown --id {\"1JF1WNS57wWcbmn1EztJxh09xU739j4_g\"} -O model.zip"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1JF1WNS57wWcbmn1EztJxh09xU739j4_g\n",
            "To: /content/SpeechSplit/assets/model.zip\n",
            "253MB [00:01, 152MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcfWB5CJnwkx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "929d20dd-a64e-4d9d-96cd-c0bd5253f186"
      },
      "source": [
        "!gdown --id {\"1SZPPnWAgpGrh0gQ7bXQJXXjOntbh4hmz\"} -O autovc.ckpt"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1SZPPnWAgpGrh0gQ7bXQJXXjOntbh4hmz\n",
            "To: /content/SpeechSplit/assets/autovc.ckpt\n",
            "341MB [00:02, 220MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WiR8MGvcnSv8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9577b536-4914-4a44-9d48-cfa261bc4c09"
      },
      "source": [
        "!unzip model.zip"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  model.zip\n",
            "  inflating: 640000-P.ckpt           \n",
            "  inflating: 660000-G.ckpt           \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m92MYkV8ll63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c2a5b11-8c4d-46c1-ec41-3bcc74351497"
      },
      "source": [
        "pip install wavenet_vocoder==0.1.1"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wavenet_vocoder==0.1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/0a/da/47da119dbd3cfc0c80b75270e4bc7b49b678bd94600928fa243922ad65bc/wavenet_vocoder-0.1.1.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from wavenet_vocoder==0.1.1) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from wavenet_vocoder==0.1.1) (1.4.1)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from wavenet_vocoder==0.1.1) (1.7.0+cu101)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->wavenet_vocoder==0.1.1) (3.7.4.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->wavenet_vocoder==0.1.1) (0.16.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->wavenet_vocoder==0.1.1) (0.8)\n",
            "Building wheels for collected packages: wavenet-vocoder\n",
            "  Building wheel for wavenet-vocoder (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wavenet-vocoder: filename=wavenet_vocoder-0.1.1-cp36-none-any.whl size=12668 sha256=2a1bd1b80a9e990aafa40680595f7e2063ccb8829e840d4bdd95cbe35435a2cc\n",
            "  Stored in directory: /root/.cache/pip/wheels/72/fc/21/02d3785b65dd072b110b44b9df98b8cbf72a89ddea424ff0d9\n",
            "Successfully built wavenet-vocoder\n",
            "Installing collected packages: wavenet-vocoder\n",
            "Successfully installed wavenet-vocoder-0.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CwN7iUrIq6_D",
        "outputId": "ebcda54d-4369-471f-b81b-796f7095ad3e"
      },
      "source": [
        "!git clone https://github.com/auspicious3000/autovc.git"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'autovc'...\n",
            "remote: Enumerating objects: 72, done.\u001b[K\n",
            "remote: Counting objects: 100% (72/72), done.\u001b[K\n",
            "remote: Compressing objects: 100% (69/69), done.\u001b[K\n",
            "remote: Total 95 (delta 10), reused 60 (delta 3), pack-reused 23\u001b[K\n",
            "Unpacking objects: 100% (95/95), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORpxkFscpf1e",
        "outputId": "5d1184e1-2ad7-4313-fc92-67e4ec18859c"
      },
      "source": [
        "cd /content/SpeechSplit"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/SpeechSplit\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_86jxEobkZcG"
      },
      "source": [
        "# demo conversion\n",
        "import torch\n",
        "import pickle\n",
        "import numpy as np\n",
        "from hparams import hparams\n",
        "from utils import pad_seq_to_2\n",
        "from utils import quantize_f0_numpy\n",
        "from model import Generator_3 as Generator\n",
        "from model import Generator_6 as F0_Converter\n",
        "\n",
        "\n",
        "device = 'cuda:0'\n",
        "G = Generator(hparams).eval().to(device)\n",
        "g_checkpoint = torch.load('assets/660000-G.ckpt', map_location=lambda storage, loc: storage)\n",
        "G.load_state_dict(g_checkpoint['model'])\n",
        "\n",
        "P = F0_Converter(hparams).eval().to(device)\n",
        "p_checkpoint = torch.load('assets/640000-P.ckpt', map_location=lambda storage, loc: storage)\n",
        "P.load_state_dict(p_checkpoint['model'])\n",
        "\n",
        "\n",
        "metadata = pickle.load(open('assets/demo.pkl', \"rb\"))\n",
        "\n",
        "\n",
        "sbmt_i = metadata[0]\n",
        "emb_org = torch.from_numpy(sbmt_i[1]).to(device)\n",
        "x_org, f0_org, len_org, uid_org = sbmt_i[2]        \n",
        "uttr_org_pad, len_org_pad = pad_seq_to_2(x_org[np.newaxis,:,:], 192)\n",
        "uttr_org_pad = torch.from_numpy(uttr_org_pad).to(device)\n",
        "f0_org_pad = np.pad(f0_org, (0, 192-len_org), 'constant', constant_values=(0, 0))\n",
        "f0_org_quantized = quantize_f0_numpy(f0_org_pad)[0]\n",
        "f0_org_onehot = f0_org_quantized[np.newaxis, :, :]\n",
        "f0_org_onehot = torch.from_numpy(f0_org_onehot).to(device)\n",
        "uttr_f0_org = torch.cat((uttr_org_pad, f0_org_onehot), dim=-1)\n",
        "\n",
        "sbmt_j = metadata[1]\n",
        "emb_trg = torch.from_numpy(sbmt_j[1]).to(device)\n",
        "x_trg, f0_trg, len_trg, uid_trg = sbmt_j[2]        \n",
        "uttr_trg_pad, len_trg_pad = pad_seq_to_2(x_trg[np.newaxis,:,:], 192)\n",
        "uttr_trg_pad = torch.from_numpy(uttr_trg_pad).to(device)\n",
        "f0_trg_pad = np.pad(f0_trg, (0, 192-len_trg), 'constant', constant_values=(0, 0))\n",
        "f0_trg_quantized = quantize_f0_numpy(f0_trg_pad)[0]\n",
        "f0_trg_onehot = f0_trg_quantized[np.newaxis, :, :]\n",
        "f0_trg_onehot = torch.from_numpy(f0_trg_onehot).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    f0_pred = P(uttr_org_pad, f0_trg_onehot)[0]\n",
        "    f0_pred_quantized = f0_pred.argmax(dim=-1).squeeze(0)\n",
        "    f0_con_onehot = torch.zeros((1, 192, 257), device=device)\n",
        "    f0_con_onehot[0, torch.arange(192), f0_pred_quantized] = 1\n",
        "uttr_f0_trg = torch.cat((uttr_org_pad, f0_con_onehot), dim=-1)    \n",
        "\n",
        "\n",
        "conditions = ['R', 'F', 'U', 'RF', 'RU', 'FU', 'RFU']\n",
        "spect_vc = []\n",
        "with torch.no_grad():\n",
        "    for condition in conditions:\n",
        "        if condition == 'R':\n",
        "            x_identic_val = G(uttr_f0_org, uttr_trg_pad, emb_org)\n",
        "        if condition == 'F':\n",
        "            x_identic_val = G(uttr_f0_trg, uttr_org_pad, emb_org)\n",
        "        if condition == 'U':\n",
        "            x_identic_val = G(uttr_f0_org, uttr_org_pad, emb_trg)\n",
        "        if condition == 'RF':\n",
        "            x_identic_val = G(uttr_f0_trg, uttr_trg_pad, emb_org)\n",
        "        if condition == 'RU':\n",
        "            x_identic_val = G(uttr_f0_org, uttr_trg_pad, emb_trg)\n",
        "        if condition == 'FU':\n",
        "            x_identic_val = G(uttr_f0_trg, uttr_org_pad, emb_trg)\n",
        "        if condition == 'RFU':\n",
        "            x_identic_val = G(uttr_f0_trg, uttr_trg_pad, emb_trg)\n",
        "            \n",
        "        if 'R' in condition:\n",
        "            uttr_trg = x_identic_val[0, :len_trg, :].cpu().numpy()\n",
        "        else:\n",
        "            uttr_trg = x_identic_val[0, :len_org, :].cpu().numpy()\n",
        "                \n",
        "        spect_vc.append( ('{}_{}_{}_{}'.format(sbmt_i[0], sbmt_j[0], uid_org, condition), uttr_trg ) )       "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kab6zhq22iSx"
      },
      "source": [
        "!touch /content/SpeechSplit/assets/autovc/__init__.py"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-36KxVns3eFd",
        "outputId": "387ea121-d924-4ab9-b9dd-5bd5103591d1"
      },
      "source": [
        "cd /content/SpeechSplit"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/SpeechSplit\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgw_ounM7l6K"
      },
      "source": [
        "!python /content/SpeechSplit/assets/autovc/hparam.py"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Azkg-IowkZcO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "d7dc3f19-0377-46ba-b867-986c44cde732"
      },
      "source": [
        "# spectrogram to waveform\n",
        "import torch\n",
        "import librosa\n",
        "import pickle\n",
        "import os\n",
        "from assets.autovc.synthesis import build_model, wavegen\n",
        "#from  os\n",
        "#from speechsplit.assets.autovc. import synthesis\n",
        "from assets.autovc import *\n",
        "if not os.path.exists('results'):\n",
        "    os.makedirs('results')\n",
        "\n",
        "model = build_model().to(device)\n",
        "checkpoint = torch.load(\"assets/checkpoint_step001000000_ema.pth\")\n",
        "model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "\n",
        "for spect in spect_vc:\n",
        "    name = spect[0]\n",
        "    c = spect[1]\n",
        "    print(name)\n",
        "    waveform = wavegen(model, c=c)   \n",
        "    librosa.output.write_wav('results/'+name+'.wav', waveform, sr=16000)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-0cbf66398692>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'results'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"assets/checkpoint_step001000000_ema.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"state_dict\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/SpeechSplit/assets/autovc/synthesis.py\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     model = getattr(builder, hparam.builder)(\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mout_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_channels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mlayers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'HParams' object has no attribute 'builder'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwmDEH_HsJTQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LKiyfuTy8Gm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}